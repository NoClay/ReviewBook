我们需要知道TCP在网络OSI的七层模型中的第四层——Transport层，IP在第三层——Network层，ARP在第二层——Data Link层，在第二层上的数据，我们叫Frame，在第三层上的数据叫Packet，第四层的数据叫Segment，我们程序的数据首先会打到TCP的Segment中，然后TCP的Segment会打到IP的Packet中，然后再打到以太网Ethernet的Frame中，传到对端后，各个层解析自己的协议，然后把数据交给更高层的协议处理。

## TCP头格式
![](http://oqnfoupsj.bkt.clouddn.com/17-8-15/49250250.jpg)
需要注意以下几点：
- TCP的包是没有IP地址的，那是IP层上的事。但是有源端口和目标端口
- 一个连接需要五元组来表示是同一个连接(src_ip,src_port,dst_ip,dst_port,协议)
- 上图中有四个非常重要的东西：
  * Sequence Number是包的序号，用来解决网络包乱序的问题
  * Acknowledgement Number就是ACK，用于确认收到，用来解决丢包的问题
  * Window又叫Advertised-Window，也就是著名的滑动窗口，用于进行流量控制的。
  * TCP Flag，也就是包的类型，用于操控TCP的状态机的
其他的东西，可以看下图：
![](http://oqnfoupsj.bkt.clouddn.com/17-8-15/96723679.jpg)

## 1.TCP的三次握手、四次挥手
![](http://oqnfoupsj.bkt.clouddn.com/17-8-9/73226225.jpg)
### TCP三次握手四次挥手时序图
![](http://oqnfoupsj.bkt.clouddn.com/17-8-9/41420069.jpg)
### TCP三次握手
要弄清TCP建立连接需要几次交互，我们需要弄清楚连接进行初始化的目标是什么，TCP进行握手初始化一个连接的目标是：分配资源、初始化序列号(通知peer对端我的初始序列号是多少,这个序列号要作为以后数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输问题而乱序)，知道初始化连接的目标，那么要达成这个目标就简单了，握手的过程可以简化为下面的三次交互：
1. Client端首先发送一个SYN告诉Server端我的初始序列号是X。
2. Server端收到SYN包后回复给Client一个ACK确认包+SYN包，告诉Client我收到了你的序列号，我的初始序列号是Y。
3. Client端收到后，回复Server端一个ACK确认包，告诉Server端我知道你的端口号了。

经历了上面三个过程初始化过程就完成了。
大部分情况下建立连接需要三次握手，也不一定都是三次，当两端同时发起SYN来建立连接时，就出现了四次握手来建立连接的情况，如下图所示。
![](http://oqnfoupsj.bkt.clouddn.com/17-8-9/82476817.jpg)
前两次握手：双方同时发起SYN请求，双方此时都收到了对方的初始序列号。
后两次握手：双方都要反馈给对方一个ACK确认包和SYN，当双方收到这次反馈后，便两次收到和发出了自己的初始序列号将这两次系列号作比较便可得知是否是对方发来的消息，因此不再需要再次反馈给对面自己确认收到了消息。
### TCP的四次挥手
TCP断开连接的目标是：回收资源、终止数据传输。由于TCP是全双工的，需要两端分别各自拆除自己通向对端方向的通信信道。这样就需要四次挥手来分别拆除通信信道：
1. Client发送一个FIN包来告诉Server我已经没数据发给Server了
2. Server收到后回复一个ACK确认包说我知道了，此时Client已经不能主动向Server端发送数据了，但是仍然可以接收来自Server端的数据；
3. 然后Server在自己也没数据发送给Client后，Server也发送一个FIN包给Client，告诉Client我也已经没数据发给Client了
4. Client收到后，就会回复一个ACK确认包说我知道了。

经过这样的四次挥手的过程，这个TCP连接就可以完全拆除了。

## 2.TCP连接的初始化序列号能否固定？
不能固定，假设ISN(初始化序列号)固定是1，Client和Server建立好一条TCP连接后，Client连续给Server发了10个包，这10个包不知怎么被链路上的路由器缓存了(路由器会毫无先兆地缓存或者丢弃任何的数据包)，这个时候碰巧Client挂掉了，然后Client用同样的端口号重新连上Server，Client又连续给Server发了几个包，假设这个时候Client的序列号变成了5。接着，之前被路由器缓存的10个数据包全部被路由到Server端了，Server给Client回复确认号10，这个时候，Client整个都不好了，这是什么情况？我的序列号才到5，你怎么给我的确认号是10了，整个都乱了。

在RFC793中，建议ISN和一个假的时钟绑定在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始，这需要4个小时的时间才会产生ISN回绕的问题，这几乎可以保证每个新连接的ISN和旧的连接的ISN产生冲突。但是这种递增方式的ISN，很容易让攻击者猜测到TCP连接的ISN，现在的实现大多是在一个基准值的基础上进行随机的。

## 3.初始化连接的SYN超时问题
Client发送SYN包给Server后挂了，Server回给Client的SYN-ACK一直没收到Client的ACK确认，这个时候这个链接既没建立起来，也不能算失败。这就需要一个超时时间让Server将这个连接断开，否则这个链接就会一直占用Server的SYN连接队列中的一个位置，大量这样的连接就会将Server的SYN连接队列耗尽，让正常的连接无法得到处理。目前，Linux下默认会进行5次重发SYN-ACK包，重试间隔时间从1s开始，下次重试间隔时间是前一次的双倍，5次的重试时间间隔是1s，2s，4s，8s，16s，总共31s，第5次发出后还要等32s才知道第5次是否超时，所以总共需要1+2+4+8+16+32 = 63s，TCP才会把这个连接断开。由于，SYN超时需要63s，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称SYN flood攻击)，用于耗尽Server的SYN队列。对于SYN过多的问题，linux提供了几个TCP参数：tcp_syncookies、tcp_synack_retries(用来减少重试次数)、tcp_max_syn_backlog(增大SYN连接数)、tcp_abort_on_overflow(当处理不过来时，直接拒绝连接)来调整应对。

tcp_syncookies：当SYN队列满了以后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发回去(又叫Cookie)，如果是攻击者则不会有响应，如果是正常连接，则会把这个SYN Cookie发回来，然后服务端可以通过Cookie建立连接(即使你不在SYN队列中)。请注意千万别tcp_syncookies来处理正常的大负载连接情况。因为syncookies是妥协式的TCP协议，并不严谨，对于正常的请求，你应该调整其他的三个参数来解决。

[SYN攻击防范技术](http://blog.sina.com.cn/s/blog_68158ebf0100uwnl.html)
## 4.TCP的两端同时断开连接
- TCP的A端在收到对端B的FIN包前发出了FIN包，那么该端A的状态就变成了FIN_WAIT1,
    * A在FIN_WAIT1状态下收到对端B对自己的FIN包的ACK包的话，那么A状态就变成FIN_WAIT2,
    * A在FIN_WAIT2状态下收到对端Peer的FIN包，在确认已经收到了对端B的全部数据包后，就响应一个ACK给对端，然后自己进入TIME_WAIT状态；
- TCP的A端在收到对端B的FIN包前发出了FIN包，那么该端A的状态就变成了FIN_WAIT1,
    * A在FIN_WAIT1状态下收到对端B的FIN包的话，那么该端B在确认已经收到了对端B的全部数据包后，就响应一个ACK给对端B，然后自己进入CLOSEING状态
    * A在CLOSEING状态下，收到自己的FIN包的ACK包的话，那么就进入TIME_WAIT状态。

因此，当TCP的两端同时发起FIN包进行断开连接，那么两端可能出现完全一样的状态转移FIN_WAIT1->CLOSEING->TIME_WAIT,也就会Client和Server最后同时进入TIME_WAIT状态。图示如下：
![](http://oqnfoupsj.bkt.clouddn.com/17-8-9/70583643.jpg)
## 5.四次挥手能不能变成三次挥手呢？
答案是可能的。TCP是全双工通信，Client在自己已经不会再有新的数据要发送给Server后，可以发送FIN信号告知Server，这边已经终止Client到对端Server的数据传输。但是这个时候对端Server可以继续往Client那边发送数据包。因此，两端数据传输的终止在时序上是独立并且会相隔较长时间的，这个时候就至少需要4次挥手来完全终止这个连接。但是，如果Server在收到Client的FIN包后，再也没数据需要发送给Client了，那么对Client的ACK包和Server自己的FIN包就可以合并为一个包发送过去，这样四次挥手就可以变成三次了(似乎Linux协议栈就是这样实现的)
## 6.TCP的头号疼症TIME_WAIT状态
### 疼症描述
四次挥手过程中，首先断开连接的一端，在回复最后一个ACK后，为什么要进行TIME_WAIT呢(超时设置是 2*MSL，RFC793定义了MSL为2分钟，Linux设置成了30s)，在TIME_WAIT的时候又不能释放资源，白白让资源占用那么长时间，能不能省了TIME_WAIT呢，为什么？

要说明TIME_WAIT的问题，需要解答一下几个问题

1.两端，哪一端会进入TIME_WAIT呢？为什么？
TCP主动关闭连接的那一方最后会进入TIME_WAIT。那么怎么界定主动关闭方呢？是否主动关闭是由FIN包的先后决定的，在自己没有收到对端的FIN包之前自己发出了FIN包，那么就是自己主动关闭链接的那一方。对于情景4中描述的情况，两端都是主动关闭方，两边都会进入TIME_WAIT。为什么是主动关闭的一方进入TIME_WAIT呢，被动关闭的进入TIME_WAIT可以不呢？我们来看看TCP四次挥手可以简单分为下面三个过程。
1. 主动关闭方发送FIN；
2. 被动关闭方收到主动关闭方的FIN后发送该FIN的ACK，被动关闭方发送FIN；
3. 主动关闭方收到被动关闭方的FIN后发送该FIN的ACK，被动关闭方等待自己FIN的ACK

原因就是过程三，根据TCP协议规范，不对FIN进行ACK，如果主动关闭方不进入TIME_WAIT，那么主动关闭方在发送完ACK就走了的话，如果最后发送的ACK在路由过程中丢掉了，最后没能发送到被动关闭方，这个时候被动关闭方没有收到自己FIN的ACK就不能关闭链接，接着被动关闭方会超时重发FIN包，但是这个时候已经没有对端会给该FIN回ACK，被动关闭方就无法正常关闭连接了，所以主动关闭方需要进入TIME_WAIT以便能够重发丢掉的被动关闭方FIN的ACK。

2.TIME_WAIT状态是用来解决或避免什么问题的？
1. 主动关闭方需要进入TIME_WAIT以便能够重发丢掉的被动关闭方FIN包的ACK。在主动关闭方发送的ACK丢失了的时候，被动关闭方由于没有收到自己FIN的ACK，会进行重传FIN包，这个FIN包传到主动关闭方后，由于这个连接已经不存在于主动关闭方了，这个时候主动关闭方无法识别这个FIN包，协议栈会认为对方疯了，都还没建立连接你给我一个FIN包？于是回复了一个RST包给被动关闭方，被动关闭方就会收到一个错误(我们见得比较多的:connect reset by peer)，这里顺便说下Broken pipe，在收到RST包的时候，还往这个连接写数据，就会收到Broken pipe错误)，原本正常关闭的链接，给我个错误，很让人难受。
2. 防止已经断开的连接1中在链路中残留的FIN包终止掉新的连接2(重用了连接1的所有的5个元素(源IP、目的IP、TCP、源端口、目的端口))，这个概率比较低，因为涉及到一个匹配问题，迟到的FIN分段的序列号必须落在连接2一方的期望序列号范围之内，虽然概率低，但是确实可能发生，因为初始序列号都是随机产生的，并且这个序列号是32位的，会回绕。
3. 防止链路上已经关闭的连接的残余数据包()干扰正常数据包，造成数据流的不正常。这个问题和2类似。

3.TIME_WAIT会带来哪些问题呢？
TIME_WAIT带来的问题是源于：一个连接进入TIME_WAIT状态后需要等待2*MSL(一般是1到4分钟)那么长的时间才能断开连接释放连接占用的资源，会造成以下问题：
1. 作为服务器，短时间内关闭了大量的Client连接，就会造成服务器上出现大量的TIME_WAIT连接，占据大量的tuple，严重消耗着服务器的资源。
2. 作为客户端，短时间内大量的短连接，会大量消耗Client机器的端口，毕竟端口只有65535个，端口被耗尽了，后续就无法再发起新的连接了。

由于上面两个问题，作为客户端需要连接本机的一个服务的时候，首选UNIX域套接字而不是TCP

如何解决或缓解TIME_WAIT问题呢？

4.TIME_WAIT的快速回收和重用
* TIME_WAIT快速回收

linux下开启TIME_WAIT快速回收需要同时打开tcp_tw_recycle和tcp_timestamps(默认开启)两个选项。linux下快速回收的时间为3.5*RTO(重传超时时间)，而一个RTO时间为200ms至120s。开启快速回收TIME_WAIT，可能会出现一些问题，为了避免这些问题，要求同时满足以下三种情况的新连接要被拒绝掉：
  1.  来自同一个对端Peer的TCP包携带了时间戳
  2. 之前同一台peer机器(仅仅识别IP地址，因为连接被快速释放了，没了端口信息)的某个TCP数据在MSL(最长报文段寿命)秒之内到过本Server
  3. Peer机器新连接的时间戳小于peer机器上次TCP到来时的时间戳，且差值大于重放窗口戳(TCP_PAWS_WINDOW)

初看起来正常的数据包同时满足上面三条几乎是不可能的，因为机器的时间戳不可能倒流的，出现上述的3点均满足时，一定是老的重复数据包又回来了，丢弃老的SYN包是正常的。到此，似乎启动快速回收就能很大程度缓解TIME_WAIT带来的问题。但是，这里忽略了一个东西就是NAT(中文意思是"网络地址转换"，它是一种把内部私有网络地址(IP地址)翻译成合法网络IP地址的技术)，在一个NAT后面的所有Peer机器在Server看来都是一个机器，NAT后面那么多Peer机器的系统时间戳很可能不一致，有些快，有些慢。这样，在Server关闭了与系统时间戳快的Client的连接后，在这个连接进入快速回收的时候，同一NAT后面的系统时间戳慢的Client向Server发起连接，这就很有可能同时满足上面的三种情况，造成该连接被Server拒绝掉。所以，在是否开启tcp_tw_recycle需要慎重考虑了。
* TIME_WAIT重用

linux上比较完美的实现了TIME_WAIT重用问题。只要满足下面两点中的一点，一个TIME_WAIT状态的四元组(即一个socket连接)可以重新被到来的SYN连接使用：
  1. 新连接SYN告知的初始化序列号比TIME_WAIT老连接的末序列号大
  2. 如果开启了tcp_timestamps，并且新到来的连接的时间戳比老连接的时间戳大

要同时开启tcp_tw_reuse选项和tcp_timestamps选项才可以开启TIME_WAIT重用，还有一个条件是：收到最后一个包后超过1s。细心的同学可能发现TIME_WAIT重用对Server端来说并没解决大量TIME_WAIT造成的系统资源消耗问题，因为不管TIME_WAIT连接是否被重用，它依旧占用着系统资源。即便如此，TIME_WAIT重用还是有些用处的，他解决了整机范围拒绝接入的问题，虽然一般一个单独的Client是不可能在MSL(最长报文段寿命)内用同一个端口连接同一个服务的，但是如果Client做了bind端口那就是同一个端口了。时间戳重用TIME_WAIT连接的机制的前提是IP地址的唯一性，得出新请求发起自同一台机器，但是如果是NAT环境下就不能这么保证了，因此，在NAT环境下，TIME_WAIT重用还是有风险的。
有些同学可能会混淆tcp_tw_reuse和SO_REUSEADDR选项，认为是相关的一个东西，其实这两个东西完全不同，tcp_tw_reuse是内核选项，而SO_REUSEADDR是用户态的选项，使用SO_REUSEADDR告诉内核，如果端口忙，但TCP状态位于TIME_WAIT，可以重用端口。如果端口忙，而TCP状态位于其他状态，重用端口时依旧得到一个错误信息，指明Address already in use。如果你的服务程序停止后想立即重启，而新套接字依旧使用同一端口，此时SO_REUSEADDR选项非常有用。但是，使用这个选项也会有危险，虽然发生概率不大。

5.清掉TIME_WAIT的技巧
可以用下面两种方式控制服务器的TIME_WAIT的数量
1. 修改tcp_max_tw_buckets：tcp_max_tw_buckets控制并发的TIME_WAIT的数量，默认值时180000.如果超过默认值，内核会把多的TIME_WAIT连接清掉，然后在日志里打一个警告。官方文档说这个选项只是为了阻止一些简单的DoS攻击，平常不要人为的降低它。
2. 利用RST包从外部清除掉TIME_WAIT链接：根据TCP规范，收到任何的发送到未侦听端口、已经关闭的连接的数据包、连接处于任何非同步状态(LISTEN、SYS-SEND、SYN-RECEIVED)并且收到的包的ACK在窗口外，或者安全层不匹配，都要回执以RST响应(而收到滑动窗口外的序列号的数据包，都要丢弃这个数据包，并回复一个ACK包)，内核收到RST将会产生一个错误并终止该链接。我们可以利用RST包来终止掉处于TIME_WAIT状态的连接，其实这就是所谓的RST攻击了。

为了描述方便：假设Client和Server有个连接Connect1，Server主动关闭连接并进入了TIME_WAIT状态，我们来描述一下怎么从外部使得Server的处于TIME_WAIT状态的连接Connect1提前终止掉。要实现这个RST攻击，首先我们要知道Client在Connect1中的端口port1(一般这个端口是随机的，比较难猜测到，这也是RST攻击比较难的一个点)，利用IP_TRANSPARENT这个socket选项，它可以bind不属于本地的地址。因此可以从任意机器绑定Client地址以及端口port1，然后向Server发起一个连接，Server收到了窗口外的包于是响应了一个ACK，这个ACK包会路由到Client处，这个时候99%的可能Client已经释放连接Connect1，这个时候Client收到这个ACK包，会发送一个RST包，Server收到RST包后就释放连接Connect1提前终止TIME_WAIT状态了。提前终止TIME_WAIT状态可能会带来危害，具体可看RFC1337.RFC1337中建议，不要用RST过早的结束TIME_WAIT状态。
## 7.TCP的延时确认机制
按照TCP协议，确认机制是累积的，也就是确认号X的确认指示的是所有X之前但不包括X的数据已经收到了。确认号ACK本身就是不含数据的分段，因此大量的确认号消耗了大量的带宽，虽然大多数情况下，ACK还是可以和数据一起捎带传输的，但是如果没有捎带传输，那么就只能单独回来一个ACK，如果这样的分段太多，网络的利用率就会下降。为了缓解这个问题，RFC建议了一种延迟的ACK，也就是说，ACK在收到数据后并不马上回复，而是延迟一段可以接受的时间，延迟一段时间的目的是看能不能和接收方要发给发送方的数据一起回去，因为TCP协议头中总是包含确认号的，如果能的话，就将数据一起捎带回去，这样网络利用率就提高了。延迟ACK就算没有数据捎带，那么如果收到了按序的两个包，那么只要对第二个包进行确认即可，这样也能省去一个ACK的消耗。由于TCP协议不需要对ACK进行ACK，RFC建议最多等待2个包的累计确认，这样能及时通知对端Peer，我这边的接收情况。Linux实现中，有延迟ACK和快速ACK，并根据当前的包的收发情况来在这两种ACK中切换。一般情况下，ACK并不会对网络性能有太大的影响，延迟ACK能减少发送的分段，从而节省了带宽，而快速ACK能及时通知发送方丢包，避免滑动窗口，提升吞吐率。关于ACK分段，有个细节需要说明一下，ACK的确认号，是确认按序收到的最后一个字节序，对于乱序到来的TCP分段，接收端会回复相同的ACK分段，只确认按序到达的最后一个TCP分段。TCP连接的延迟确认时间一般初始化为最小值40ms，随后根据连接的重传超时时间(RTO)、上次收到的数据包与本次接收数据包的间隔等参数进行不断调整。
## 8.TCP的传送机制以及重传的超时计算
1. TCP的重传超时计算：TCP交互的过程中，如果发送的包一直都没收到ACK确认，显然不能一直等，因为如果发送的包在路由过程中丢失了，对端是没办法给你发送确认消息的，这样协议将不可用，既然不能一直等下去，那么该等多久呢？

- 等太长时间的话，数据包都丢了很久才重发，没有效率，性能差；
- 等太短时间的话，可能ACK还在路上快到了，这个时候却重传了，造成浪费，同时过多的重传会造成网络堵塞，进一步加剧数据的丢失。

我们应该通过一个合适的算法去计算这个重传超时时间，这个时间应该是随着网络的状况在变化的。

为了使我们的重传机制更高效，如果我们能够比较准确的知道在当前网络状况下，一个数据包从发出去到回来的时间RTT，那么根据这个RTT我们就可以方便的设置TimeOut---RTO(超时重传时间)，以让我们的重传机制更高效。 听起来似乎很简单，好像就是在发送端发包时记下t0，然后接收端再把这个ack回来时再记一个t1，于是RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。

算法1：为了计算这个RTO，RFC793中定义了一个经典算法，如下：
1. 首先采样计算RTT值，计算最近几次的RTT值
2. 然后计算平滑RTT，称为(其中α是一个滑动因子取值在0.8到0.9之间，叫加权移动平均)SRTT = (α\*SRTT)+((1-α)\*RTT)
3. RTO = min(UBOUND,max(LBOUND,(β*SRTT)))

- UBOUND是最大的TimeOut时间，例如：可以定义为1分钟
- LBOUND是最小的TimeOut时间，例如：可以定义为1s
- β是一个延迟方差因子，例如：1.3-2.0

然而这个算法有个缺点就是：在算RTT样本时，是用第一次发数据的时间和ACK回来的时间做RTT样本值，还是用重传的时间和ACK回来的时间做RTT的样本值？不管怎样选择，总会造成要么把RTT算过长了，要么把RTT算过短了。

情况（a）是ack没回来，所以重传。如果你计算第一次发送和ACK的时间，那么，明显算大了。

情况（b）是ack回来慢了，但是导致了重传，但刚重传不一会儿，之前ACK就回来了。如果你是算重传的时间和ACK回来的时间的差，就会算短了。
![](http://oqnfoupsj.bkt.clouddn.com/17-8-10/36920673.jpg)
算法2：Karn / Partridge 算法

针对上面算法的缺陷，有人对上面的算法进行了改进(忽略了重传，不把重传的RTT做采样)，但是这个算法仍然存在问题：如果在某一时间，网络闪动，突然变慢了，产生了较大的延时，这个延时导致要重传所有的包(因为之前的RTO很小)，于是，因为重传的不算，所以RTO就不会被更新，这是一个灾难。于是Karn算法用了一个取巧的方式——只要一发生重传，就对现有的RTO值翻倍（这就是所谓的 Exponential backoff），很明显，这种死规矩对于一个需要估计比较准确的RTT也不靠谱。

算法3:Jacobson / Karels 算法

前两种算法用的都是"加权滑动平均"，这种方法最大的毛病就是如果RTT有一个大的波动的话，很难被发现，因为被平滑掉了。于是为了解决上面两个算法的问题，又有人提出了一个新算法，名叫Jacobson/Karels算法(参看RFC6289)，这个算法引入了最新的RTT的采样和平滑过的SRTT的差距做因子来计算。这个算法的核心是：除了考虑每两次的测量值的偏差之外，其变化率也应该考虑在内，如果变化率过大，则通过以变化率为自变量的函数为主计算RTT(如果突然增大，则取值为比较大的正数，如果突然减小，则取值为比较小的负数，然后和测量平均值加权求和)，反之如果变化率很小，则取测量平均值。公式如下：(其中DevRTT是偏差RTT的意思)
1. SRTT = SRTT + α(RTT-SRTT)   计算平滑RTT
2. DevRTT = (1-β)\*DevRTT + β\*(|RTT-SRTT|)   计算平滑RTT和真实的差距(加权移动平均)
3. RTO = μ*SRTT + ∂\*DevRTT           神一样的公式

其中：在Linux下，α = 0.125，β = 0.25， μ = 1，∂ = 4这就是算法中的"调的一手好参数",nobody knows why, it just works)最后的这个算法在被用在今天的TCP协议中并工作的非常好。

直到了计算算法后，就应该考虑定时器的设计问题了，一个简单直观的方案就是为TCP中的每一个数据包维护一个定时器，在这个定时器到期前没收到确认，则进行重传。这种设计理论上是很合理的，但是实现上，这种方案会有非常多的定时器，会带来巨大的内存开销和调度开销。既然不能每个包都有一个定时器，那么多少个包一个定时器好呢，这个似乎韩那确定，其实可以换个思路去考虑，不要以包量来确定定时器，以连接来确定定时器会不会比较合理呢？目前，采取每一个TCP连接单一超时定时器的设计则成了一个默认的选择，并且RFC2988给出了每连接单一定时器的设计建议算法规则：
1. 每一次一个包含数据的包被发送(包括重发)，如果还没开启重传定时器，则开启它，使得它在RTO秒之后超时(按照当前的RTO值)
2. 当接收到一个ACK确认一个新的数据，如果所有的发出数据都被确认了，关闭重传定时器
3. 当收到一个ACK确认一个新的数据，还有数据在传输，也就是还有没有被确认的数据，重新启动重传定时器，使得它在RTO秒后超时(按照当前的RTO值)

当重传定时器超时后，依次做下面三件事情：

4.1重传最早的尚未被TCP接收方ACK的数据包

4.2重新设置RTO为RTO\*2(还原定时器)，但是新的RTO不应该超过RTO的上限(RTO有个上限值，这个上限值是60s)

4.3重启重传定时器

上面的算法体现了一个原则：没被确认的包必须可以超时，并且超过的时间不能太长，同样也不要过早重传。规则[1][3][4.3]共同说明了只要还有数据包没被确认，那么定时器一定会是开启着的(这样才满足没被确认的包必须可以超时的原则)。规则[4.2]说明定时器的超时值是有上限的(满足超时的时间不能太长).规则[3]说明，在一个ACK到来后重置定时器可以保护后发的数据不被过早的重传；因为一个ACK到来了，说明后续的ACK可能会依次到来，也就是说丢失的可能性并不大。规则[4.2]也是一定程度上避免了过早重传，因为，在出现定时器超时后，也有可能是网络出现阻塞了，这个时候应该延长定时器，避免出现大量的重传进一步加剧网络的拥塞。

2.TCP的重传机制
TCP的重传是由超时触发的，这会引发一个重传选择问题，假设TCP发送端连续发了1、2、3、4、5、6、7、8、9、10共10包，其中4、6、8这三个包全丢失了，由于TCP的ACK是确认最后连续收到的序号，这样发送端只能收到3号包的ACK，这样在TIME_OUT的时候，发送端就面临下面两个重传选择：
1. 仅重传4号包
2. 重传3号包后面的所有包，也就是4~10号包

对于两个选择优缺点都比较明显。
方案1：
  - 优点：按需重传，能够最大程度的节省带宽。
  - 缺点：重传会比较慢，因为重传4号包后，需要等待下一次超时时才会重传6号包

方案2：
  - 优点：重传较快，数据能够较快交付给接收端。
  - 缺点：重传了很多不必要重传的包，浪费带宽，在出现丢包时，一般是网络拥塞，大量的重传有可能进一步加剧拥塞。

上述问题出现的原因都是因为单纯以时间驱动来进行重传的，都必须等待一个超时时间，不能快速对当前网络状况作出响应，如果加入以数据驱动呢？
### 快重传
如果包没有连续到达，就ack最后那个可能被丢了的包，如果发送端连续收到三次相同的ACK，就重传，而不用等待timeout再重传。
比如：如果发送方发出了1，2，3，4，5份数据，第一份先到送了，于是就ack回2，结果2因为某些原因没收到，3到达了，于是还是ack回2，后面的4和5都到了，但是还是ack回2，因为2还是没有收到，于是发送端收到了三个ack=2的确认，知道了2还没有到，于是就马上重转2。然后，接收端收到了2，此时因为3，4，5都收到了，于是ack回6。示意图如下：
![](http://oqnfoupsj.bkt.clouddn.com/17-8-15/61465676.jpg)

快速重传解决了timeout的问题，但是没解决重传一个还是重传多个的问题。出现难以决定是否重传多个包问题的根源在于，发送端不知道哪些非连续序号的包已经到达接收端了，但是接收端是知道的，如果接收端告诉一下发送端不就可以了，于是RFC2018提出了SACK选择确认机制，在Linux下，可以通过tcp_sack参数打开这个功能(Linux2.4后默认开启)，SACK是TCP的扩展项，包括(1)SACK允许选项(Kind=4,Length=2，选项只允许在有SYN标志的TCP包中)，(2)SACK信息选项(Kind=5,Length)，一个SACK的例子如下图，红框说明：接收端收到了0-5500,8000-8500,7000-7500,6000-6500的数据了，这样发送端就可以选择重传丢失的5500-6000,6500-7000,7500-8000的包
![](http://oqnfoupsj.bkt.clouddn.com/17-8-15/44006937.jpg)

![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/95872111.jpg)
接收方有权把已经传给发送端SACK里的数据给丢弃，去吧内存留给更重要的东西，所以发送方也不能完全依赖SACK，还是要依赖ACK，并维护timeout，如果后续的ACK没有增长，还是要把SACK的东西重传，另外，接收端这边永远不能把SACK的包标记为ACK.

SACK会消费发送方的资源，如果一个攻击者给数据发送方发了一堆SACK的选项，这会导致发送方开始要重传甚至遍历已经发出的数据，这会导致消耗很多发送端的资源。

示例一：ACK丢包

下面的示例中，丢了两个ACK，所以，发送端重传了第一个数据包（3000-3499），于是接收端发现重复收到，于是回了一个SACK=3000-3500，因为ACK都到了4000意味着收到了4000之前的所有数据，所以这个SACK就是D-SACK——旨在告诉发送端我收到了重复的数据，而且我们的发送端还知道，数据包没有丢，丢的是ACK包。

Transmitted  Received    ACK Sent
Segment      Segment     (Including SACK Blocks)

3000-3499    3000-3499   3500 (ACK dropped)
3500-3999    3500-3999   4000 (ACK dropped)
3000-3499    3000-3499   4000, SACK=3000-3500

示例二，网络延误

下面的示例中，网络包（1000-1499）被网络给延误了，导致发送方没有收到ACK，而后面到达的三个包触发了“Fast Retransmit算法”，所以重传，但重传时，被延误的包又到了，所以，回了一个SACK=1000-1500，因为ACK已到了3000，所以，这个SACK是D-SACK——标识收到了重复的包。

这个案例下，发送端知道之前因为“Fast Retransmit算法”触发的重传不是因为发出去的包丢了，也不是因为回应的ACK包丢了，而是因为网络延时了。

Transmitted    Received    ACK Sent
Segment        Segment     (Including SACK Blocks)

500-999        500-999     1000
1000-1499      (delayed)
1500-1999      1500-1999   1000, SACK=1500-2000
2000-2499      2000-2499   1000, SACK=1500-2500
2500-2999      2500-2999   1000, SACK=1500-3000
1000-1499      1000-1499   3000

引入D-SACK，有这么几个好处：
1. 可以让发送方知道，是发出去的包丢了，还是回来的ACK包丢了
2. 是不是自己的timeout太小了，导致重传
3. 网络上出现了先发的包后到的情况(又称reordering)
4. 网络上是不是把我的数据包给复制了。

Linux下的tcp_dsack参数用于开启这个功能(Linux2.4后默认打开)

SACK依靠接收端的接收情况反馈，解决了重传风暴问题，这样够了吗？接收端能不能反馈更多的信息呢？显然是可以的，于是，RFC2883对SACK进行了扩展，提出了D-SACK，也就是利用第一块SACK数据描述重复接收的不连续数据块的序列号参数，其他的SACK数据则描述其他正常接收到的不连续数据。这样发送方利用第一块SACK，可以发现数据段被网络复制，错误重传，ACK丢失引起的重传、重传超时等异常的网络状况，使得发送端能更好的调整自己的重传策略，使得发送端能更好的调整自己的重传策略。D-SACK的优点：
  - 发送端可以判断出，是发包丢失了，还是接收端的ACK丢失了。(发送方，重传了一个包，发现并没有D-SACK那个包，那么就是发送的数据包丢失了，否则就是接收端的ACK丢失了，或者是发送的包延迟到达了)
  - 发送端可以判断自己的RTO是不是有点小了，导致过早重传(如果接收到比较多的D-SACK就该怀疑是RTO小了)
  - 发送端可以判断自己的数据包是不是被复制了。(如果明明没有重传该数据包，但是接收到该包的D-SACK)
  - 发送端可以判断目前网络上是不是出现了有些包被delay了，也就是出现先发的包确后到了。
## 9.TCP的流量控制

### 9.1滑动窗口
TCP必须要解决的可靠传输以及包乱序的问题，所以TCP必须要知道网络实际的数据处理带宽或是数据处理速度，这样才不会引起网络拥塞，导致丢包。TCP头里有一个字段叫Window，又叫Advertised-Window，这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。为了说明滑动窗口，我们需要先看一下TCP缓冲区的一些数据结构:
![](http://oqnfoupsj.bkt.clouddn.com/17-8-17/25939805.jpg)
上图中，我们可以看到：
- 接收端LastByteRead指向了TCP缓冲区中读到的位置，NextByteExpected指向的地方是接收到的连续包的最后一个位置，LastByteRcved指向的是收到的包的最后一个位置，我们看到中间有些数据还没有到达，所以有数据空白区。
- 发送端的LastByteAcked指向了指向了被接收端Ack过的位置(表示成功发送确认)，LastByteSent表示发出去了，但还没有收到成功确认的Ack，LastByteWritten指向的是上层应用正在写的地方。

于是:
- 接收端在給发送端回Ack中会汇报自己的Advertised-Window = MaxRcvBuffer-LastByteRcvd-1(缓存可用空间)
- 而发送方会根据这个窗口来控制发送数据的大小，以保证接收方可以处理。

下面我们来看一下滑动窗口的示意图：
![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/1394070.jpg)
上图中分为4部分，分别是(其中那个黑模型就是滑动窗口)：
1. \#1已收到ack确认的数据。
2. \#2已发送还没收到ack的数据
3. \#3在窗口中还没有发出的(接收方还有空间)
4. \#4窗口以外的数据(接收方没有空间)



TCP的窗口是一个16bit位的字段，它代表的是窗口的字节容量，也就是TCP的标准窗口最大为2^16-1 = 65535个字节。另外在TCP选项字段中还包含了一个窗口扩大因子，option-kind为3，option-length为3个字节，option-data取值范围是0-14。窗口扩大因子用来扩大TCP窗口，可把原来16bit的窗口，扩大为31bit。这个窗口是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。也就是，发送端是根据接收端通知的窗口大小来调整自己的发送速率的，以达到端到端的流量控制。尽管流量控制看起来简单明了，就是根据接收端的限制来控制自己的发送就好了，但是这还是会产生许多问题：
1. 发送端是怎么做到比较方便的知道自己哪些包可以发，哪些包不能发呢？
2. 如果接收端通知一个零窗口给发送端，这个时候发送端还能不能发送数据呢？如果不发数据，那一直等待接收端口通知一个非0窗口吗？如果接收端一直不通知呢？
3. 如果接收端处理能力很慢，这样接收端的窗口很快会被填满，然后接收处理完几个字节，腾出几个字节的窗口后，通知发送端，这个时候发送端马上就发送几个字节给接收端吗？发送的话会不会太浪费了，就像一艘万吨油轮只装上几斤油就开去目的地一样，对于发送端产生数据的能力很弱也一样，如果发送端慢吞吞产生几个字节的数据要发送，这个时候该不该立即发送呢？还是积累多一点再发送？

### 疑问1的解决：
发送方要知道哪些可以发，哪些不能发，一个简明的方案就是按照接收方的窗口通告，发送方维护一个一样大小的发送窗口就可以了，在窗口内的可以发，窗口外的不可以发，窗口在发送序列上不断后移，这就是TCP中的滑动窗口。如下图所示，对于TCP发送端其发送缓存内的数据都可以分为4类：
1. 已经发送并得到接收端ACK的；
2. 已经发送但还未收到接收端ACK的；
3. 未发送但允许发送的(接收方还有空间)
4. 未发送且不允许发送(接收方没空间了)；

其中2，3两部分合起来称之为发送窗口
![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/1394070.jpg)
下面两图演示的窗口的滑动情况，收到36的ACK后，窗口向后滑动5个byte
![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/8250917.jpg)
![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/68099163.jpg)
### 疑问2的解决：
由问题1我们知道，发送端的发送窗口是由接收端控制的，下图，展示了一个发送端是怎么受接收端控制的：
![](http://oqnfoupsj.bkt.clouddn.com/17-8-13/93995277.jpg)
由上图我们知道，当接收端通知一个zero窗口的时候，发送端的发送窗口也变成了0，也就是发送端不能发数据了，如果发送端一直等待，直到接收端通知一个非0窗口再发数据的话，这就太受限于接收端了，如果接收端一直不通知新的窗口，也不能干等，因此这里就引入了一个主动探测的机制。为了解决0窗口的问题，TCP采用了Zero Window Probe(0窗口探测)技术，缩写为ZWP。发送端在窗口变成0后，会发ZWP包给接收方，让接收方来ACK它的Window尺寸，来探测目前接收端窗口的大小，一般这个值会设置成3次，每次大约(30-60s)(不同的实现可能会不同)，如果三次后还是0的话，有的TCP实现就会发RST(连接重置)断掉这个连接。

注意：只要有等待的地方就有可能出现DDos攻击，Zero Window也不例外。一些攻击者会在和HTTP建好链发完GET请求后，就把Window设置为0，然后服务端就只能等待进行ZWP，于是攻击者会并发大量这样的请求，把服务端的资源耗尽。
### 疑问3的解决
糊涂窗口综合症：如果接收方太忙，来不及取走Receive Window里的数据，那么就会导致发送方发送的数据包越来越小。到最后，如果接收方腾出几个字节并告诉发送方现在有几个字节的Window，而发送方就会义无反顾的发送这几个字节(要知道我们的TCP+IP头有40个字节，为了几个字节，要搭上这么大的开销，这太不经济了)。

网络上有个MTU，对于以太网来说，MTU是1500字节，除去TCP-IP头的40个字节，真正的数据传输可以有1460，这就是所谓的MSS(Max Segment Size)注意，TCP的RFC定义这个MSS的默认值时536，这是因为RFC791里说了任何一个IP设备都得最少接收576尺寸的大小(实际上来说576是拨号的网络的MTU，而576减去IP头的20个字节就是536)

如果你的网络包可以塞满MTU，那么你可以用满整个带宽，如果不能，那么你就会浪费带宽(大于MTU的包有两种结局，一种是直接被丢弃了，另一种是会被重新分块打包发送)你可以想象成一个MTU就相当于一个飞机最多可以装的人，如果这飞机里满载的话，带宽最高，如果一个飞机只运一个人的话，无疑成本增加了。

这个问题本质上是一个避免发送大量小包的问题。造成这个问题的原因有二：1)接收端一直在通知一个小的窗口2)发送端本身的问题，一直在发送小包。这个问题，在TCP中的术语叫糊涂窗口综合症。解决这个问题的思路有两个:1)接收端不通知小窗口；2)发送端积累一下数据再发送。

思路1:在接收端解决这个问题，如果收到的数据导致window size小于某个值，就ACK一个0窗口，这就阻止了发送端在发数据过来。等接收端处理了一些数据后windows Size大于等于了MSS(最大传输大小)，或者buffer有一半为空，就可以告知一个非0窗口。

思路2是在发送端解决这个问题，即Nagle算法：
1. 如果包长度达到MSS(最大传输大小)，则允许发送
2. 如果该包含有FIN，则允许发送
3. 设置了TCP_NODELAY选项，则允许发送；
4. 设置TCP_CORK选项时，若所有发出去的小数据包(包长度均小于MSS)均被确认，则允许发送。
5. 上述条件都未满足，但发生了超时(一般为200ms)，则立即发送。

Nagle算法是默认打开的，所以，对于一些小包场景的程序：比如:像talnet或ssh这样的交互性比较强的程序，你需要关闭这个算法。你可以在socket设置TCP_NODELAY选项来关闭这个算法(关闭Nagle算法没有全局参数，需要根据每个应用自己的特点来关闭)

```
setsockopt(sock_fd, IPPROTO_TCP, TCP_NODELAY, (char *)&value,sizeof(int));

```
另外，网上有些文章说TCP_CORK的socket option也可以关闭Nagle算法，这是不对的，TCP_CORK其实是更激进的Nagle算法，完全禁止小包发送，而Nagle算法没有禁止小包发送，只是禁止了大量的小包发送，最好不要两个选项都设置。

规则4指出TCP连接上最多只能有一个未被确认的小数据包。从规则4可以看出Nagle算法并不禁止发送小的数据包(超时时间内)，而是避免发送大量小的数据包。由于Nagle算法是依赖ACK的，如果ACK很快的话，也会出现一直发小包的情况，造成网络利用率低。TCP_CORK选项则是禁止发送小的数据包(超时时间内)，设置该选项后，TCP会尽力把小数据包拼接成一个大的数据包(一个MTU)再发出去，当然也不会一直等，发生了超时(一般为200ms)，也立即发送。Nagle算法和TCP_CORK选项提高了网络的利用率，但是增加了延时。从规则3可以看出，设置TCP_NODELAY选项是，完全禁用Nagle算法了。

这里要说一个小插曲，Nagle算法和延迟确认一起，当出现(write-write-read)的时候回引发一个40ms的延时问题，这个问题在HTTP svr中体现的比较明显，场景如下：

客户端在请求下载HTTP svr中的一个小文件，一般情况下，HTTP svr都是先发送HTTP响应头，然后再发送HTTP响应BODY(特别是比较多的实现在发送文件的实施采用的是sendfile系统调用，这就出现write-write-read模式了)。当发送头部时，由于头部较小，于是形成了一个小的TCP包发送到客户端，这个时候开始发送body，由于body也较小，这样还是形成一个小的TCP数据包，根据Nagle算法，HTTP svr已经发送了一个小的数据包了，在收到第一个小包的ACK后或等待200ms超时后才能再发小包，HTTP svr不能发送这个body小TCP包

客户端在收到http响应头后，由于这是一个小的TCP包，于是客户端开启延时确认，客户端在等待svr的第二个包老在一起确认或等待一个超时(一般是40ms)在发送ACK包；这样就出现了你等我，然而我也在等你的死锁情况，于是出现最多的情况时客户端等待一个40ms的超时，然后发送ACK给HTTP svr，HTTP svr 收到ACK包后再发送body部分，大家在测HTTP svr的时候就要留意这个问题了。
## 10.TCP的拥塞控制
如果网络上的延时突然增加，那么，TCP对这个事做出来的应对只有重传数据，但是，重传会导致网络的负担更重，于是会导致更大的延迟以及更多的丢包，于是，这个情况就会进入恶性循环被不断地放大。试想一下，如果一个网络内有成千上万的TCP连接都这么行事，那么就会形成"网络风暴"，TCP这个协议就会拖垮整个网络。这是一个灾难。

所以，TCP不能忽略网络上发生的事情，而无脑的一个劲重发数据，对网络造成更大的伤害。对此TCP的设计理念是：TCP不是一个自私的协议，当拥塞发生时，要做自我牺牲。就像交通阻塞一样，每个车都应该把路让出来，而不要再去抢路了。

拥塞控制主要是四个算法：1)慢启动2)拥塞避免3)拥塞发生4)快速恢复
### 慢热启动算法
慢启动的算法如下：
1. 连接建好的开先始初始化cwnd=1,表明可以传一个MSS大小的数据。
2. 每当收到一个ACK，cwnd++;呈线性上升
3. 每当过了一个RTT(RTT是客户到服务器往返所花时间),cwnd = cwnd*2；呈指数上升
4. 还有一个sshthresh(slow start threshold),是一个上限，当cwnd >= sshthresh时，就会进入"拥塞避免算法"。

所以我们看到，如果网速很快的话，ACK也会返回的快，RTT也会短，这个慢启动就一点也不慢。下图说明了这个过程。
![](http://oqnfoupsj.bkt.clouddn.com/17-8-17/54346229.jpg)
Linux3.0后把cwnd初始化成了10MSS。在Linux3.0以前，比如206,Linux采用了RFC3390,cwnd是跟MSS的值来变的，如果MSS<1095,则cwnd=4;如果MSS>2190，则cwnd = 2；其他情况下，则是3。

### 拥塞避免算法
一般来说sshthresh的值是65535，单位是字节，当cwnd达到这个值后，算法如下：
1. 收到一个ACK时，cwnd = cwnd + 1/cwnd
2. 当每过一个RTT时，cwnd = cwnd + 1

这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。很明显，是一个线性上升的算法。

### 拥塞状态时的算法
前面我们说过，当丢包的时候，会有两种情况：

1)等到RTO超时，重传数据包，这种情况其实很不好
- sshthresh = cwnd/2
- cwnd重置为1
- 进入慢启动过程
2)快重传算法，也就是在收到三个重复ACK时就开启重传，而不用等到RTO超时。
- TCP Tahoe的实现和RTO超时一样
- TCP Reno的实现是：
  + cwnd = cwnd/2
  + sshthresh = cwnd
  + 进入快速恢复算法

上面我们可以看到RTO超时后，sshthresh会变成cwnd的一半，这意味着，如果cwnd <= sshthresh时出现丢包，那么TCP的sshthresh就会减了一半，然后等cwnd又很快的以指数级增长爬到这个地方时，就会成慢慢的线性增长。我们可以看到，TCP是怎么通过这种强烈地震荡快速而小心的找到网站流量的平衡点的。

### 快速恢复算法
TCP Reno这个算法定义在RFC5681。快速重传和快速恢复算法一般同时使用。快速恢复算法是认为：你还有3个重复ACK说明网络也不是那么糟糕，所以没有必要像RTO超时那么强烈。正如前面所说的，进入快恢复前，cwnd和sshthresh已被更新：
- cwnd = cwnd/2
- sshthresh = cwnd

然后真正的快恢复算法如下：
- cwnd = sshthresh + 3*MSS(3的意思是确认有三个数据包被收到了)
- 重传重复ACK指定的数据包
- 如果再收到了重复的ACK，那么cwnd = cwnd+1
- 如果收到了新的ACK，那么，cwnd=sshthresh,进入拥塞避免算法。

其实这个算法也存在问题，那就是它依赖于3个重复的Ack。注意，3个重复的Ack并不代表只丢了一个数据包，很有可能是丢了好多包。但这个算法只会重传一个，剩下的那些包只能等到RTO超时，于是，进入噩梦模式：超时一个窗口就减半一下，多个超时会造成TCP的传输速度呈指数级下降，也不会触发快回复算法。

通常来说，SACK和D-SACK的方法可以让快回复或发送端在做决定时更聪明一些，但是并不是所有的TCP的实现都支持SACK(SACK需要两端都支持)，所以，需要一个没有SACK的解决方案。而通过SACK进行拥塞控制的算法是FACK.

TCP New Reno

1995年，TCP New Reno算法提出，主要是在没有SACK的支持下改进快回复算法：
- 当发送端这边收到了3个重复算法，进入快重传模式，开始重传重复Ack指示的那个包。如果只有这一个包丢了，那么，重传这个包后回来的Ack会把整个已经被发送端传输出去的数据Ack回来，如果没有的话，说明有多个包丢了。我们叫这个ACK为PACK
- 一旦发送端这边发现了PACK出现了，那么，发送端就可以知道由多个包丢了，于是继续重传滑动窗口里未被ACK的第一个包。直到再也收不到PACK，才真正结束快回复这个过程。

我们可以看到，这个"快恢复的变更"是一个非常激进的玩法，它同时延长了快重传和快恢复的过程。
### 算法示意图
![](http://oqnfoupsj.bkt.clouddn.com/17-8-17/9567009.jpg)
### FACK算法
- 会把SACK中最大的Sequence Number保存在snd.fack这个变量，snd.fack的更新由ack







## 12.附加题2:系统调用listen()的backlog参数指的是什么？
要说明backlog参数的含义，首先需要说一下Linux的协议栈维护的TCP连接的两个连接队列:[1]SYN半连接队列；[2]accept连接队列
1. SYN半连接队列：Server端收到Client的SYN包并回复SYN+ACK包后，该连接的信息就会被移到一个队列，这个队列就是SYN半连接队列(此时TCP连接处于非同步状态)
2. accept连接队列：Server端收到SYN+ACK包的ACK包后，就会将连接信息从1中的队列移到另外一个队列，这个队列就是accept连接队列(这个时候TCP连接已经建立,三次握手完成了)

用户进程调用accept()系统调用后，该连接信息就会从2中的队列中移走。

在linux Kernel2.2之前backlog指的是1和2两个队列的和。而2.2以后，指的就是2的大小，那么在2.2以后，1的大小如何确定呢？两个队列的作用又分别是什么呢？

### SYN半连接队列
对于SYN半连接队列的大小是由(/proc/sys/net/ipv4/tcp_max_syn_backlog)这个内核参数控制的，有些内核似乎也受listen的backlog参数影响，取得是两个值的最小值。当这个队列满了，Server会丢弃新来的SYN包，而Client端多次重发SYN包得不到响应而返回(connection time out)错误。但是，当Server端开启了syncookies,那么SYN半连接队列就没有逻辑上的最大值了，并且/proc/sys/net/ipv4/tcp_max_syn_backlog设置的值也会被忽略。
### accept连接队列
accept连接队列的大小是由backlog参数和(/proc/sys/net/core/somaxconn)内核参数共同决定的，取值为两个中的最小值。当accept连接队列满了，协议栈的行为根据(/proc/sys/net/ipv4/tcp_abort_on_overflow)内核参数而定。如果tcp_abort_on_overflow=1，Server在收到SYN_ACK的ACK包后，协议栈会丢弃该连接并回复RST包给对端，这个Client会出现(connection rest by peer)错误，如果tcp_abort_on_overflow=0，Server在收到SYN_ACK的ACK包后，直接丢弃该ACK包。这个时候Client认为连接已经建立了，一直在等Server的数据，直到出现read timeout错误。
